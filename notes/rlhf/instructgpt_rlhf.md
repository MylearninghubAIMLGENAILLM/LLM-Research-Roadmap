# RLHF (Ouyang et al., 2022)
**Problem:** Align models with human preferences.

**Key Idea:** Pretrain → SFT → Reward Model → PPO.

**Results:** Better helpfulness, safety, alignment.

**Reflection:** Key milestone in alignment research.
