# InstructGPT (Ouyang et al., 2022)
**Problem:** GPT-3 outputs often unaligned with human intent.

**Key Idea:** Use human feedback + RLHF.

**Results:** InstructGPT is more aligned and safer.

**Reflection:** RLHF is central to alignment research.
