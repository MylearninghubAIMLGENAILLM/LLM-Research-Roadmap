# Chinchilla (Hoffmann et al., 2022)
**Problem:** GPT-3 used too few tokens relative to parameters.

**Key Idea:** Train smaller models with more tokens.

**Results:** Chinchilla outperforms GPT-3 at same compute.

**Reflection:** Data is as important as parameters.
