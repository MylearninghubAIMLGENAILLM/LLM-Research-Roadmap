# Attention Is All You Need (Vaswani et al., 2017)
**Problem:** Seq2Seq models rely heavily on recurrence.

**Key Idea:** Replace recurrence with self-attention.

**Results:** Transformer achieves SOTA in translation.

**Reflection:** Basis for GPT, BERT, and modern LLMs.
