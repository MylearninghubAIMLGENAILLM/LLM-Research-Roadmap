# Quantization Techniques
- **GPTQ (Frantar et al., 2023):** Accurate post-training quantization.
- **AWQ (Lin et al., 2023):** Activation-aware quantization.
- **AQLM (Dettmers et al., 2023):** Low-bit post-training quantization.

**Reflection:** Quantization reduces inference costs.
